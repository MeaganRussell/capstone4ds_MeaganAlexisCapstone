---
title: "Predicting Flood Severity of Northwest Florida Counties using Random Forest Regression Modeling - Spring 2026 "
author: "Meagan Russell (Advisor: Dr. Cohen)"
date: last-modified
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
<!--

This is for me later, once the slides are ready to go.

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)
-->

## Introduction

Flooding is caused by many mechanisms, including when either heavy rainfall causes rivers to overflow, when impervious surfaces reduce water infiltration, and/or when soil becomes oversaturated (@Javadinejad2022). Flash floods are considered to be one of the most severe natural disasters due to their rapid onset, unpredictability, infrastructure damage, and associated fatalities (@Diakakis2020). Infrastructure and agricultural damage costs have steadily increased in the twentieth century (@Saharia2017). According to the IPCC, the severity and frequency of flood-causing severe storms will likely increase due to climate change (@IPCC2021Chapter11). Due to Northwest Florida’s location relative to the Gulf of Mexico and major hurricane paths, most flooding research has focused on sea-level rise and coastal flooding, leaving inland flood risk understudied (@Bilskie2014Dynamics; @Morss2024Storm). Growing evidence of machine learning models' capabilities to accurately predict storm severity and susceptibility highlights a major opportunity to apply these models to Northwest Florida (@Gensini2021SevereML; @Akshay2021FloodSeverity; @Chen2022RandomForestGeo).

Random Forest (RF) is a type of machine learning algorithm that predicts the dependent variable based on independent variables (@Salman2024RandomForest). Errors produced by the random forest training can be counteracted by increasing the number of samples and trees of classification (@Wang2015FloodRF). Additionally, each decision tree’s selection of data is random, which prevents the model from memorizing the data (@Salman2024RandomForest). The RF application allowed for the importance of each variable to be assessed, and each index contribution to the overall risk was calculated pertaining to the relevant study of interest in China (@Wang2015FloodRF). The RF technique is optimal for performing hydrological studies due to the ability of the machine learning model to map flood risk analysis for hazard assessment with a low-cost application of the non-traditional model compared to rainfall-runoff models (@Zhu2022FloodRF).  For instance,  a machine learning study in 2024 using 16 flood risk factors (FRFs), including historic flood damage locations, found that several high and very high flood risk areas closely overlapped with FEMA’s 100-year floodplain in Tampa Bay (@Dey2024). The random forest model was able to reproduce the characteristics of the studied flood events and represented the daily flood discharge well in comparison with the hydromad modeling (@Schoppa2020RFDischarge). RF is a smart model that uses simple models, aka decision trees (@Salman2024RandomForest). 

Northwest Florida, often referred to as the Florida Panhandle, is home to 16 counties: Escambia, Santa Rosa, Okaloosa, Walton, Holmes, Washington, Bay, Jackson, Calhoun, Gulf, Liberty, Franklin, Gadsden, Leon, Wakulla, and Jefferson. Since January 1996, these counties have had 1,052 floods, 790 of which were flash floods and 262 non-flash floods. Flooding has resulted in approximately $656 million in property damage throughout the counties, with Escambia County ranking number one in damage with approximately $152 million since 1996. This study will be utilizing RF to predict flood severity in Northwest Florida based on an index created using the number of injuries, number of deaths, time of flooding event, type of flood (flash and non-flash), and property damage. This type of RF modeling has not been used at a county level in Northwest Florida and most flooding research is focused on coastal flooding due to the counties being in major hurricane paths. This is important because floods are not uniform for every county and mitigation planning is an important part of preparing for natural hazards. 
















## Week 1 - Individual Research:

**Paper One: Random Forest Algorithm Overview**

This paper provides an overview of the Random Forest (RF) algorithm. Random Forest is a type of machine learning algorithm that uses a target variable known as a dependent variable. The goal of Random Forest is to predict the dependent variable based on independent variables. The way RF works is by creating decision trees. Decision trees are a series of yes/no questions, for example: 


Did the flooding cause any deaths? 

Did the flooding result in more than $1,000,000 in property damage?

Did the flood occur countywide?

Did the flood cause agricultural damage?


The authors go on to explain how Random Forest is a smart model that uses simple models, decision trees, which each can make small mistakes, but in the end, once all of the simple models are combined, they get the answer right. In addition, each decision tree's selection of data is random, which prevents the model from memorizing the data. There are limitations to Random Forest, such as increased computational cost and reduced transparency; however, overall Random Forest is reliable, widely applicable, and suitable for real-world problems.

Cite new paper @Salman2024RandomForest

**Paper two: Machine Learning Classification of Significant Tornadoes and Hail in the United States Using ERA5 Proximity Soundings**

In this paper the authors compared logistic regression, gaussian naive bayes, support vector machines, decision trees, adaptive boosting, gradient boosting, and random forest classification to see which model could best predict the severity class of tornadoes and hail events (severe vs. significant-severe). The data used in this study was collected from two sources: ERA5 for the weather and climate and NOAA for the actual storm events. The goal was to predict the severity of the storms with the weather and climate data before known events. To test the models the authors used validation techniques, multiple skill metrics, and frequency. For instance, the authors tested the models by leaving one year out for cross validation and they evaluated model performance using the critical success index (CSI). Of the models used, Random Forest, was the best model. It was the most accurate and it didn't over or under estimate the severity of the storms. The authors also state that Random Forest can potentially be used for real-world forecasting applications.

Cite new paper @Gensini2021SevereML

## Week 2: Individual Research:

**Paper one: Predicting Flood Severity Using Machine Learning and Hybrid Models** 

This paper compares linear regression, support vector machines (svm), random forest, xgboost, and a hybrid model of the models to determine which model/hybrid model is the best at predicting the severity of flood damage. The authors used hictorical flood records to determine severity. They used Total Damage as the main variable to predict severity. The authors sorted the floods by total damage, smallest amount to largest, and then split the groups evenly. The groups were defined as low-severity, medium-severity, and high-severity. To verify that this classification of severity was accurate the authors looked at the number of deaths, number of displaced individuals, and the number of people affected. They found that the number of those three variables increased with the severity class already defined. The authors trained the models to evaulate which model/hybrid model was the most accurate at predicting severity. The authors found that the hybrid model that combined random forest, support vector machine, and xgboost was the most accurate, while XGBoost came in second and Random Forest was third. Overall, the study found that combining models resulted in a more accurate prediction, high-severity floods were eaiser to classify than moderate-severity floods, and flood severity can be predicted using machine learning.

Cite new paper @AkshayFloodSeverity

**Paper two: The value of advanced flood models, damage costs and land use data in cost-effective climate change adaptation** 

The authors in this paper had a goal of evaluating how different flood modelling approaches and damage cost models would affect the estimates of coastal flood damage. The authors compare your dynamic and static flood models and high-resolution local damage data versus standardized damage curves. The goal was to see how much model choice alone would affect and or change the flooded area estimates, damage costs, and cost-effective adaptation decisions. This research is important because damage cost estimates differ even when they're under the same climate scenario and different models can predict different flooded areas. The authors had two case studies on two Danish coastal cities that had different topography. One was Odense which had a low which is low lying and it had a river and forged system. And then Aabenraa has a steep terrain but it's near the coast. The authors used three flood modeling approaches, static EU DEM model, static Danish DHYM model, and dynamic DH flood risk model. They used three climate scenarios present day, RCP 4.5, and RCP 8.5. The authors found that the static flood models would often overestimate the flooded area and the dynamic models showed smaller but more realistic flooding extents. They found that the DTU model had a higher per unit damage cost, the EU model had a larger flooded area but lower per unit cost. With any research, the authors spoke about how model uncertainty remained and that there were damages that are underrepresented (cultural heritage, ecosystem services, and long-term health impacts). Overall, this paper and this research showed that model choice mattered just as much as climate change itself and that combining dynamic flood models with detailed local damage data would produce more reliable inputs for cost effective climate adaptation decisions.

Cite new paper @Halsnaes2023FloodModels
